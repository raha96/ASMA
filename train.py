# -*- coding: utf-8 -*-
"""Phase2_model_tests.ipynb

Modified version based on the original automatically generated by Colaboratory.

Original file is located at
        https://colab.research.google.com/drive/1vMqJJZGBBSvNek7qYCyYgp0MYIr6fCjb
"""

from keras import Sequential
import keras.layers
import keras.losses
import sklearn.metrics
import pickle
import numpy as np

logfile = open("action.log", "w")

input_layer = 4000

model = Sequential([
        keras.layers.Input (shape=(input_layer,)),
        keras.layers.Dense (units=64, activation="relu"),
        #keras.layers.Dense (units=64, activation="relu"),
        keras.layers.Dense (units=1, activation="sigmoid")
])

print(model.summary())

training_folders = ['91', '74', '122', '76', '54', '224', '108', '80', '72',
                    '3', '218', '189', '167', '159', '139', '88', '60', '33',
                    '21', '18', '231', '230', '190', '186', '182', '178', '177',
                    '156', '154', '144', '141', '133', '109', '99', '86', '83',
                    '77', '75', '68', '66', '65', '64', '58', '37', '32', '24',
                    '15', '13', '12', '5', '240', '239', '238', '237',
                    '236', '233', '226', '223', '220', '216', '215', '213',
                    '212', '208', '206', '204', '202', '201', '200',
                    '199', '198', '196', '195', '194', '193', '192',
                    '185', '184', '183', '179', '174', '172', '171',
                    '169', '168', '164', '163', '161', '160', '155',
                    '153', '148', '147', '142', '140', '138', '137',
                    '135', '132', '131', '128', '124', '118', '117',
                    '116', '115', '113', '112', '111', '105', '103', '102',
                    '101', '98', '96', '93', '92', '87', '85', '84', '82',
                    '78', '71', '70', '67', '63', '61', '57', '53',
                    '46', '45', '44', '40', '39', '38', '35', '34', '31',
                    '29', '28', '27', '26', '25', '23', '14', '11',
                    '10', '9', '7', '4', '2', '0']
test_folders = ['187', '1', '8', '30', '22', '222', '94', '79', '36', '234',
                    '203', '175']

"""Create a class_weight dictionary that can be passed on to keras' fit function
to mitigate for the imbalance in the training dataset"""
def calc_class_weight(train_y) -> dict:
    count_0, count_1 = 0, 0
    for item in train_y:
        if item == 0:
            count_0 += 1
        elif item == 1:
            count_1 += 1
        else:
            assert False
    if count_0 > count_1:
        out = {0: 1, 1: count_0 / count_1}
    else:
        out = {0: count_1 / count_0, 1: 1}
    print(f"0: {count_0}, 1: {count_1}")
    print(out)
    return out

"""# Long version"""

with open("dataset/output.pickle", "rb") as fin:
    raw_data = pickle.load(fin)

train_in_list, train_out_list = [], []
test_in_list, test_out_list = [], []

for folder in training_folders:
    fdata = raw_data[folder]
    for fname in fdata:
        inp = fdata[fname]["features"][0]
        out = int(int(fdata[fname]["label"]["num_statemachines"]) > 0)
        train_in_list.append(inp)
        train_out_list.append(out)

for folder in test_folders:
    fdata = raw_data[folder]
    for fname in fdata:
        inp = fdata[fname]["features"][0]
        out = int(int(fdata[fname]["label"]["num_statemachines"]) > 0)
        test_in_list.append(inp)
        test_out_list.append(out)

print(train_in_list[:5])
print(train_out_list[:5])
print(test_in_list[:5])
print(test_out_list[:5])

# Pad the datapoints
maxlen = 0
for folder in raw_data:
    for fname in raw_data[folder]:
        feature_vector = raw_data[folder][fname]["features"][0]
        maxlen = max(maxlen, len(feature_vector))

print(f"Maximum length of a feature vector is {maxlen}.")

for i in range(len(test_in_list)):
    len_diff = input_layer - len(test_in_list[i])
    if len_diff >= 0:
        for _ in range(len_diff):
            test_in_list[i].append(0)
    else:
        test_in_list[i] = test_in_list[i][:input_layer]

for i in range(len(train_in_list)):
    len_diff = input_layer - len(train_in_list[i])
    if len_diff >= 0:
        for _ in range(len_diff):
            train_in_list[i].append(0)
    else:
        train_in_list[i] = train_in_list[i][:input_layer]

test_in = np.array(test_in_list)
train_in = np.array(train_in_list)
test_out = np.array(test_out_list)
train_out = np.array(train_out_list)
print(f"Training input: {train_in.shape}; Training output: {train_out.shape}")
print(f"Test input: {test_in.shape}; Test output: {test_out.shape}")

model.compile(optimizer="adam", loss=keras.losses.BinaryCrossentropy(), metrics="acc")

history = model.fit(x=train_in, y=train_out, batch_size=32, epochs=50, validation_data=(test_in,test_out), class_weight=calc_class_weight(train_out))
model.save("hasfsm.keras")

y_true = test_out.astype(float)
y_pred = np.array(model(test_in))
y_pred = y_pred.reshape(y_pred.shape[0])
y_pred[y_pred >= 0.5] = 1.0
y_pred[y_pred < 0.5] = 0.0
#print(sklearn.metrics.accuracy(y_true, y_pred))
logfile.write("hasfsm.keras: \n")
metrics = sklearn.metrics.confusion_matrix(y_true, y_pred)
print(metrics)
logfile.write(f"{metrics}\n")



fp, fn, tp, tn = 0, 0, 0, 0
for i in range(len(y_true)):
    yt = y_true[i]
    yp = y_pred[i]
    if yt == 0 and yp == 0:
        tn += 1
    elif yt == 1 and yp == 0:
        fn += 1
    elif yt == 0 and yp == 1:
        fp += 1
    elif yt == 1 and yp == 1:
        tp += 1
    else:
        assert False

msg = f"Accuracy for class 0: {tn / (tn+fn)}\nAccuracy for class 1: {tp / (tp+fp)}"
print(msg)
logfile.write(f"{msg}\n")
metrics = sklearn.metrics.classification_report(y_true, y_pred)
print(metrics)
logfile.write(f"{metrics}\n")

"""# Short version"""

# Note that this version only includes records with state machines

def create_dataset(label_name:str, input_layer:int) -> tuple:
    with open("dataset/output.pickle", "rb") as fin:
        raw_data = pickle.load(fin)
    train_in_list, train_out_list = [], []
    test_in_list, test_out_list = [], []
    for folder in training_folders:
        fdata = raw_data[folder]
        for fname in fdata:
            inp = fdata[fname]["features"][0]
            out = int(int(fdata[fname]["label"][label_name]))
            fsm = bool(int(fdata[fname]["label"]["num_statemachines"]) > 0)
            if fsm:
                train_in_list.append(inp)
                train_out_list.append(out)
    for folder in test_folders:
        fdata = raw_data[folder]
        for fname in fdata:
            inp = fdata[fname]["features"][0]
            out = int(int(fdata[fname]["label"][label_name]))
            fsm = bool(int(fdata[fname]["label"]["num_statemachines"]) > 0)
            if fsm:
                test_in_list.append(inp)
                test_out_list.append(out)
    maxlen = 0
    for folder in raw_data:
        for fname in raw_data[folder]:
            feature_vector = raw_data[folder][fname]["features"][0]
            maxlen = max(maxlen, len(feature_vector))
    for i in range(len(test_in_list)):
        len_diff = input_layer - len(test_in_list[i])
        if len_diff >= 0:
            for _ in range(len_diff):
                test_in_list[i].append(0)
        else:
            test_in_list[i] = test_in_list[i][:input_layer]
    for i in range(len(train_in_list)):
        len_diff = input_layer - len(train_in_list[i])
        if len_diff >= 0:
            for _ in range(len_diff):
                train_in_list[i].append(0)
        else:
            train_in_list[i] = train_in_list[i][:input_layer]
    test_in = np.array(test_in_list)
    train_in = np.array(train_in_list)
    test_out = np.array(test_out_list)
    train_out = np.array(train_out_list)
    print(f"Training input: {train_in.shape}; Training output: {train_out.shape}")
    print(f"Test input: {test_in.shape}; Test output: {test_out.shape}")
    return train_in, train_out, test_in, test_out

def train_and_evaluate(model, train_in, train_out, test_in, test_out, filename:str):
    model.compile(optimizer="adam", loss=keras.losses.BinaryCrossentropy(), metrics="acc")
    history = model.fit(x=train_in, y=train_out, batch_size=32, epochs=50, validation_data=(test_in,test_out), class_weight=calc_class_weight(train_out))
    y_true = test_out.astype(float)
    y_pred = np.array(model(test_in))
    y_pred = y_pred.reshape(y_pred.shape[0])
    y_pred[y_pred >= 0.5] = 1.0
    y_pred[y_pred < 0.5] = 0.0
    #print(sklearn.metrics.accuracy(y_true, y_pred))
    logfile.write(f"{filename}: \n")
    metrics = f"{sklearn.metrics.confusion_matrix(y_true, y_pred)}\n{sklearn.metrics.classification_report(y_true, y_pred)}"
    print(metrics)
    logfile.write(f"{metrics}\n")
    model.save(filename)

"""# Goodness classification"""

input_layer = 1600
x_train, y_train, x_test, y_test = create_dataset("isgood", input_layer)
model2 = Sequential([
            keras.layers.Input (shape=(input_layer,)),
            keras.layers.Dense (units=128, activation="relu"),
            #keras.layers.Dense (units=16, activation="relu"),
            keras.layers.Dense (units=1, activation="sigmoid")
    ])
train_and_evaluate(model2, x_train, y_train, x_test, y_test, "good.keras")

"""# Separation classification"""

input_layer = 1600
x_train, y_train, x_test, y_test = create_dataset("transitions_separate_outputs", input_layer)
model2 = Sequential([
            keras.layers.Input (shape=(input_layer,)),
            keras.layers.Dense (units=128, activation="relu"),
            #keras.layers.Dense (units=16, activation="relu"),
            keras.layers.Dense (units=1, activation="sigmoid")
    ])
train_and_evaluate(model2, x_train, y_train, x_test, y_test, "transitions.keras")

"""# Labeled states"""

input_layer = 1600
x_train, y_train, x_test, y_test = create_dataset("labeled_states", input_layer)
model2 = Sequential([
            keras.layers.Input (shape=(input_layer,)),
            keras.layers.Dense (units=128, activation="relu"),
            #keras.layers.Dense (units=16, activation="relu"),
            keras.layers.Dense (units=1, activation="sigmoid")
    ])
train_and_evaluate(model2, x_train, y_train, x_test, y_test, "labeled.keras")

"""# Transition list sanity"""

input_layer = 1600
x_train, y_train, x_test, y_test = create_dataset("sensitivity_list", input_layer)
model2 = Sequential([
            keras.layers.Input (shape=(input_layer,)),
            keras.layers.Dense (units=128, activation="relu"),
            #keras.layers.Dense (units=16, activation="relu"),
            keras.layers.Dense (units=1, activation="sigmoid")
    ])
train_and_evaluate(model2, x_train, y_train, x_test, y_test, "sensitivity.keras")

logfile.close()